# -*- coding: utf-8 -*-
"""Copy of best_model_pro.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1LVgz-IxSpESc85_VUJEGZIZhr6oSf6s2
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
from sklearn.model_selection import train_test_split
from tensorflow.keras.preprocessing.image import ImageDataGenerator
import cv2

# Load the dataset
df = pd.read_csv('/content/drive/MyDrive/fer2013_balanced.csv')

# Check the data
print(f"Dataset shape: {df.shape}")
print(df['emotion'].value_counts())

# Emotion labels
emotion_labels = {
    0: 'Angry',
    1: 'Disgust',
    2: 'Fear',
    3: 'Happy',
    4: 'Sad',
    5: 'Surprise',
    6: 'Neutral'
}

def preprocess_data(df):
    # Convert string pixels to numpy arrays
    pixels = df['pixels'].apply(lambda x: np.array(x.split(' '), dtype='float32'))

    # Reshape to 48x48 images
    images = np.array([pixel.reshape(48, 48, 1) for pixel in pixels])

    # Normalize pixel values
    images = images / 255.0

    # Convert labels to categorical
    labels = keras.utils.to_categorical(df['emotion'].values, num_classes=7)

    return images, labels

# Preprocess the data
X, y = preprocess_data(df)

# Split the data
X_train, X_val, y_train, y_val = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=df['emotion']
)

print(f"Training set: {X_train.shape}")
print(f"Validation set: {X_val.shape}")

# Create data generators with augmentation
train_datagen = ImageDataGenerator(
    rotation_range=15,
    width_shift_range=0.1,
    height_shift_range=0.1,
    horizontal_flip=True,
    zoom_range=0.1,
    shear_range=0.1,
    fill_mode='nearest'
)

val_datagen = ImageDataGenerator()  # No augmentation for validation

train_generator = train_datagen.flow(X_train, y_train, batch_size=64)
val_generator = val_datagen.flow(X_val, y_val, batch_size=64)

def create_fer_model():
    model = keras.Sequential([
        # First Convolutional Block
        layers.Conv2D(64, (3, 3), activation='relu', padding='same',
                     input_shape=(48, 48, 1)),
        layers.BatchNormalization(),
        layers.Conv2D(64, (3, 3), activation='relu', padding='same'),
        layers.BatchNormalization(),
        layers.MaxPooling2D((2, 2)),
        layers.Dropout(0.25),

        # Second Convolutional Block
        layers.Conv2D(128, (3, 3), activation='relu', padding='same'),
        layers.BatchNormalization(),
        layers.Conv2D(128, (3, 3), activation='relu', padding='same'),
        layers.BatchNormalization(),
        layers.MaxPooling2D((2, 2)),
        layers.Dropout(0.25),

        # Third Convolutional Block
        layers.Conv2D(256, (3, 3), activation='relu', padding='same'),
        layers.BatchNormalization(),
        layers.Conv2D(256, (3, 3), activation='relu', padding='same'),
        layers.BatchNormalization(),
        layers.MaxPooling2D((2, 2)),
        layers.Dropout(0.25),

        # Fourth Convolutional Block
        layers.Conv2D(512, (3, 3), activation='relu', padding='same'),
        layers.BatchNormalization(),
        layers.Conv2D(512, (3, 3), activation='relu', padding='same'),
        layers.BatchNormalization(),
        layers.MaxPooling2D((2, 2)),
        layers.Dropout(0.25),

        # Classifier
        layers.Flatten(),
        layers.Dense(512, activation='relu'),
        layers.BatchNormalization(),
        layers.Dropout(0.5),
        layers.Dense(256, activation='relu'),
        layers.BatchNormalization(),
        layers.Dropout(0.5),
        layers.Dense(7, activation='softmax')
    ])

    return model

# Create and compile the model
model = create_fer_model()

model.compile(
    optimizer=keras.optimizers.Adam(learning_rate=0.001),
    loss='categorical_crossentropy',
    metrics=['accuracy']
)

model.summary()

# Callbacks
callbacks = [
    keras.callbacks.ReduceLROnPlateau(
        monitor='val_accuracy',
        factor=0.5,
        patience=5,
        min_lr=1e-7
    ),
    keras.callbacks.EarlyStopping(
        monitor='val_accuracy',
        patience=15,
        restore_best_weights=True
    ),
    keras.callbacks.ModelCheckpoint(
        'best_fer_model.h5',
        monitor='val_accuracy',
        save_best_only=True,
        mode='max'
    )
]

# Train the model
history = model.fit(
    train_generator,
    epochs=100,
    validation_data=val_generator,
    callbacks=callbacks,
    steps_per_epoch=len(X_train) // 64,
    validation_steps=len(X_val) // 64
)

# Load best model
best_model = keras.models.load_model('best_fer_model.h5')

# Evaluate
train_loss, train_accuracy = best_model.evaluate(X_train, y_train, verbose=0)
val_loss, val_accuracy = best_model.evaluate(X_val, y_val, verbose=0)

print(f"Training Accuracy: {train_accuracy:.4f}")
print(f"Validation Accuracy: {val_accuracy:.4f}")

# Plot training history
plt.figure(figsize=(12, 4))

plt.subplot(1, 2, 1)
plt.plot(history.history['accuracy'], label='Training Accuracy')
plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
plt.title('Model Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()

plt.subplot(1, 2, 2)
plt.plot(history.history['loss'], label='Training Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.title('Model Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()

plt.tight_layout()
plt.show()

# ========== ENHANCED EVALUATION METRICS ==========

# Get predictions
y_pred = best_model.predict(X_val)
y_pred_classes = np.argmax(y_pred, axis=1)
y_true_classes = np.argmax(y_val, axis=1)

# Import necessary libraries for enhanced evaluation
from sklearn.metrics import classification_report, confusion_matrix
import seaborn as sns

# Classification Report
print("\n" + "="*50)
print("CLASSIFICATION REPORT")
print("="*50)
print(classification_report(y_true_classes, y_pred_classes,
                          target_names=emotion_labels.values()))

# Confusion Matrix
plt.figure(figsize=(10, 8))
cm = confusion_matrix(y_true_classes, y_pred_classes)
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
            xticklabels=emotion_labels.values(),
            yticklabels=emotion_labels.values())
plt.title('Confusion Matrix')
plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.tight_layout()
plt.show()

# Calculate per-class accuracy
class_accuracy = {}
for i in range(7):
    class_mask = y_true_classes == i
    if np.sum(class_mask) > 0:
        class_accuracy[emotion_labels[i]] = np.sum(y_pred_classes[class_mask] == i) / np.sum(class_mask)

print("\n" + "="*50)
print("PER-CLASS ACCURACY")
print("="*50)
for emotion, acc in class_accuracy.items():
    print(f"{emotion}: {acc:.4f}")

# Visualize misclassified examples
misclassified_indices = np.where(y_pred_classes != y_true_classes)[0]

print(f"\nTotal misclassified samples: {len(misclassified_indices)}")
print(f"Validation accuracy: {val_accuracy:.4f}")

# Show some misclassified examples
plt.figure(figsize=(15, 10))
for i in range(min(12, len(misclassified_indices))):
    idx = misclassified_indices[i]
    plt.subplot(3, 4, i+1)
    plt.imshow(X_val[idx].squeeze(), cmap='gray')
    plt.title(f'True: {emotion_labels[y_true_classes[idx]]}\nPred: {emotion_labels[y_pred_classes[idx]]}')
    plt.axis('off')
plt.suptitle('Misclassified Examples', fontsize=16)
plt.tight_layout()
plt.show()

# Class distribution analysis
plt.figure(figsize=(12, 5))

plt.subplot(1, 2, 1)
class_distribution = df['emotion'].value_counts().sort_index()
class_distribution.index = [emotion_labels[i] for i in class_distribution.index]
class_distribution.plot(kind='bar')
plt.title('Class Distribution in Dataset')
plt.xlabel('Emotion')
plt.ylabel('Count')
plt.xticks(rotation=45)

plt.subplot(1, 2, 2)
# Accuracy per class bar plot
plt.bar(class_accuracy.keys(), class_accuracy.values())
plt.title('Accuracy per Class')
plt.xlabel('Emotion')
plt.ylabel('Accuracy')
plt.xticks(rotation=45)
plt.ylim(0, 1)

plt.tight_layout()
plt.show()

# Additional metrics
from sklearn.metrics import precision_recall_fscore_support

precision, recall, fscore, support = precision_recall_fscore_support(
    y_true_classes, y_pred_classes, average=None
)

print("\n" + "="*50)
print("DETAILED PER-CLASS METRICS")
print("="*50)
print(f"{'Emotion':<10} {'Precision':<10} {'Recall':<10} {'F1-Score':<10} {'Support':<10}")
print("-" * 50)
for i, emotion in emotion_labels.items():
    print(f"{emotion:<10} {precision[i]:<10.4f} {recall[i]:<10.4f} {fscore[i]:<10.4f} {support[i]:<10}")

# Calculate overall weighted metrics
weighted_precision, weighted_recall, weighted_fscore, _ = precision_recall_fscore_support(
    y_true_classes, y_pred_classes, average='weighted'
)

print("\n" + "="*50)
print("WEIGHTED AVERAGE METRICS")
print("="*50)
print(f"Weighted Precision: {weighted_precision:.4f}")
print(f"Weighted Recall: {weighted_recall:.4f}")
print(f"Weighted F1-Score: {weighted_fscore:.4f}")

# Save the final model
best_model.save('final_fer_model.h5')
print("\nModel saved as 'final_fer_model.h5'")